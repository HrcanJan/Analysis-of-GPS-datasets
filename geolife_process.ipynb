{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a4b8a73-cb0a-4408-9ad5-6f1306c1eead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geohash2 as gh\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88536b06-6ec6-42c3-9ca0-f19096af7b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('images/'):\n",
    "\tos.makedirs('images/')\n",
    "if not os.path.isdir('htmls/'):\n",
    "\tos.makedirs('htmls/')\n",
    "if not os.path.isdir('data/'):\n",
    "\tos.makedirs('data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66131ff1-b1e5-4a90-927f-4f2ab26a8ad5",
   "metadata": {},
   "source": [
    "# 2. Load geolife data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2906c950-0c99-4b6a-8c23-1947dcc5517a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [03:03<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "data_path = './geolife/Data/'\n",
    "\n",
    "dataframes = []\n",
    "index = 0\n",
    "\n",
    "for user_folder in tqdm(os.listdir(data_path)):\n",
    "\tuser_folder_path = os.path.join(data_path, user_folder)\n",
    "\n",
    "\tif os.path.isdir(user_folder_path):\n",
    "\t\ttrajectory_dir = os.path.join(user_folder_path, 'Trajectory')\n",
    "\t\t\n",
    "\t\tif os.path.exists(trajectory_dir) and os.path.isdir(trajectory_dir):\n",
    "\t\t\tfor trajectory_file in os.listdir(trajectory_dir):\n",
    "\t\t\t\tif trajectory_file.endswith('.plt'):\n",
    "\t\t\t\t\ttrajectory_file_path = os.path.join(trajectory_dir, trajectory_file)\n",
    "\t\t\t\t\tdf = pd.read_csv(trajectory_file_path, header=None, skiprows=6)\n",
    "\t\t\t\t\tdf.insert(0, 'person', trajectory_file.replace(\".plt\", \"\"))\n",
    "\t\t\t\t\tdf.insert(0, 'file', int(user_folder))\n",
    "\t\t\t\t\tdataframes.append(df)\n",
    "\t\t\tindex += 1\n",
    "\n",
    "gps_data = pd.concat(dataframes, ignore_index=True)\n",
    "column_names = ['Person ID','Trajectory', 'Latitude', 'Longitude', '0', 'Altitude', 'NumDays', 'Date', 'Time']\n",
    "gps_data.columns = column_names\n",
    "gps_data['Timestamp'] = pd.to_datetime(gps_data['Date'] + ' ' + gps_data['Time'])\n",
    "gps_data = gps_data.drop(columns=['NumDays', 'Date', 'Time', '0'])\n",
    "gps_data = gps_data[gps_data['Latitude'] >= 39.75]\n",
    "gps_data = gps_data[gps_data['Latitude'] <= 40.1]\n",
    "gps_data = gps_data[gps_data['Longitude'] >= 116.18]\n",
    "gps_data = gps_data[gps_data['Longitude'] <= 116.6]\n",
    "gps_data.to_csv(\"data/geolife_gps_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792486c5-67b1-4459-b35d-741b39d0bbf4",
   "metadata": {},
   "source": [
    "# 3. Load geolife labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fdaef2b-aea3-4966-98a3-64ac42906d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './geolife/Data/'\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for user_folder in os.listdir(data_path):\n",
    "\tuser_folder_path = os.path.join(data_path, user_folder)\n",
    "\n",
    "\tlabels_file_path = os.path.join(user_folder_path, 'labels.txt')\n",
    "\t\t\t\t\t\n",
    "\tif os.path.exists(labels_file_path):\n",
    "\t\tlabels_df = pd.read_csv(labels_file_path, sep='\\t')\n",
    "\t\t\n",
    "\t\tdataframes.append(labels_df)\n",
    "\n",
    "data_labels = pd.concat(dataframes, ignore_index=True)\n",
    "data_labels.to_csv(\"data/geolife_labels_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44735fef-76eb-4df4-aa80-e9310f165df0",
   "metadata": {},
   "source": [
    "# 4. Create Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97f87b7a-18fd-44b4-8cdf-a6a355c0fe1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_taxi_ids = gps_data['Person ID'].unique()\n",
    "unique_taxi_ids_df = pd.DataFrame({'Person ID': unique_taxi_ids})\n",
    "unique_taxi_ids_df = unique_taxi_ids_df.sort_values(by='Person ID')\n",
    "unique_taxi_ids_df = unique_taxi_ids_df.reset_index(drop=True)\n",
    "unique_taxi_ids_df.to_csv('data/geolife_nodes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84c87d1-6fc5-4beb-8e45-d06b5dd52a09",
   "metadata": {},
   "source": [
    "# 5. Add Geohash codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fbd2347-4336-433a-8a57-f282be01ebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_geohash(lat, lon):\n",
    "\treturn gh.encode(lat, lon, precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a57fb9f5-a963-493b-a946-1707587df1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18165801/18165801 [09:13<00:00, 32832.43it/s]\n"
     ]
    }
   ],
   "source": [
    "codes = []\n",
    "columns = gps_data.columns.to_list()\n",
    "lat = columns.index('Latitude')\n",
    "lon = columns.index('Longitude')\n",
    "for i in tqdm(range(len(gps_data))):\n",
    "\tcodes.append(create_geohash(gps_data.iat[i, lat], gps_data.iat[i, lon]))\n",
    "\n",
    "geohash_data = gps_data.copy(deep=True)\n",
    "geohash_data['Geohash'] = codes\n",
    "\n",
    "geohash_data.to_csv('data/geolife_geohash_size_8.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd34284-322c-45ea-acbf-0e4df49a4417",
   "metadata": {},
   "source": [
    "# 6. Group users based on geohash codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4726c390-68ee-4c60-8dc3-35c790933760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pairs of meetings for two persons\n",
    "def createEdges(geohash_data):\n",
    "\tlocations_meets = []\n",
    "\tprint(\"loaded\")\n",
    "\tgeohash_count = len(geohash_data['Geohash'].unique())\n",
    "\tprint(geohash_count)\n",
    "\tprint(\"grouping\")\n",
    "\tcodes = geohash_data.groupby('Geohash')\n",
    "\tprint(\"grouped\")\n",
    "\n",
    "\tt = 0\n",
    "\tk = 0\n",
    "\tfor code, group in tqdm(codes):\n",
    "\t\tt += 1\n",
    "\t\tlat = group['Latitude'].mean()\n",
    "\t\tlon = group['Longitude'].mean()\n",
    "\t\tperson_ids = group['Person ID'].values.tolist()\n",
    "\t\ttimes = group['Timestamp'].values.tolist()\n",
    "\n",
    "\t\tlocations_meets.append({\n",
    "\t\t\t'Geohash': code,\n",
    "\t\t\t\"persons\": person_ids,\n",
    "\t\t\t\"times\": times,\n",
    "\t\t\t'Latitude': lat,\n",
    "\t\t\t'Longitude': lon\n",
    "\t\t})\n",
    "\n",
    "\t\tif len(locations_meets) > 1000:\n",
    "\t\t\tif not (os.path.isdir('data/geolife_edges/')):\n",
    "\t\t\t\tos.makedirs('data/geolife_edges/')\n",
    "\n",
    "\t\t\twith open('data/geolife_edges/geolige_meets_'+str(k)+'.json', 'w') as json_file:\n",
    "\t\t\t\tjson.dump(locations_meets, json_file, indent=4)\n",
    "\n",
    "\t\t\tk += 1\n",
    "\t\t\tlocations_meets = []\n",
    "\t\n",
    "\tprint(str(t / float(geohash_count)), t, '/', geohash_count)\n",
    "\t\n",
    "\tif len(locations_meets) > 0:\n",
    "\t\tif not (os.path.isdir('data/geolife_edges/')):\n",
    "\t\t\tos.makedirs('data/geolife_edges/')\n",
    "\n",
    "\t\twith open('data/geolife_edges/geolife_meets_' + str(k) + '.json', 'w') as json_file:\n",
    "\t\t\tjson.dump(locations_meets, json_file, indent=4)\n",
    "\n",
    "\treturn k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86eb8cf9-f65d-4064-8ed5-b77ba39f1a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n",
      "421526\n",
      "grouping\n",
      "grouped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421526/421526 [01:39<00:00, 4221.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 421526 / 421526\n",
      "421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [00:10<00:00, 39.71it/s]\n"
     ]
    }
   ],
   "source": [
    "k = createEdges(geohash_data)\n",
    "print(k)\n",
    "edges = []\n",
    "\n",
    "# for i in tqdm(range(421)):\n",
    "for i in tqdm(range(k)):\n",
    "\td = pd.read_json('data/geolife_edges/geolige_meets_' + str(i) + '.json')\n",
    "\tedges.append(d.copy(deep=True))\n",
    "edges = pd.concat(edges)\n",
    "edges.to_csv('data/geolife_edges_size_8.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f2e65-0620-4a25-a071-226b171c617b",
   "metadata": {},
   "source": [
    "# 7. Geohash meets without time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a971791-0e70-41e1-9cfc-9da9538b64c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meet_geohash():\n",
    "\tedges_all = pd.read_csv('data/geolife_edges_size_8.csv')\n",
    "\tpersons = []\n",
    "\tp_times = []\n",
    "\tcount_p = []\n",
    "\tcount_t = []\n",
    "\tt_diff = []\n",
    "\t\n",
    "\tfor i in tqdm(range(len(edges_all))):\n",
    "\t\tx = json.loads(edges_all.iloc[i]['times'].replace(\"'\",\"\\\"\"))\n",
    "\t\tp = json.loads(edges_all.iloc[i]['persons'])\n",
    "\t\tassert len(x) == len(p)\n",
    "\t\tt = {}\n",
    "\t\tfor j in np.unique(p):\n",
    "\t\t\tt[str(j)] = []\n",
    "\t\tfor j in range(len(x)):\n",
    "\t\t\tt[str(p[j])].append(x[j])\n",
    "\t\tpersons.append(json.dumps(np.unique(p).tolist()))\n",
    "\t\tp_times.append(json.dumps(t))\n",
    "\t\tcount_p.append(len(np.unique(p)))\n",
    "\t\tcount_t.append(len(x))\n",
    "\t\txt = np.array(x, dtype='datetime64[s]')\n",
    "\t\tt_diff.append(abs(np.timedelta64(xt.max() - xt.min(), 's').astype('int')))\n",
    "\n",
    "\tmeet_edges = edges_all.copy(deep=True)\n",
    "\tmeet_edges['persons'] = persons\n",
    "\tmeet_edges['times'] = p_times\n",
    "\tmeet_edges['count_p'] = count_p\n",
    "\tmeet_edges['count_t'] = count_t\n",
    "\tmeet_edges['diff_time'] = t_diff\n",
    "\n",
    "\tmeet_edges.to_csv(\"data/geolife_groupby_geohash_size_8.csv\", index=False)\n",
    "\tmeet_edges[(meet_edges['diff_time'] >= 0) & (meet_edges['count_p'] > 1)].to_csv(\"data/geolife_meet_geohash_size_8.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67fa9747-5d5e-4b97-b58b-c6c3f444c374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421421/421421 [01:04<00:00, 6565.02it/s]\n"
     ]
    }
   ],
   "source": [
    "create_meet_geohash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0775b59-d573-4d3c-8fc4-60f99a38c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meet_edges_without_time():\n",
    "\n",
    "\tprint('loading')\n",
    "\tmeet_geohash = pd.read_csv(\"data/geolife_meet_geohash_size_8.csv\")\n",
    "\tmeets = {}\n",
    "\tprint('starting')\n",
    "\tfor i in tqdm(range(len(meet_geohash))):\n",
    "\t\tp = json.loads(meet_geohash.iloc[i]['persons'])\n",
    "\t\tfor a in range(len(p)):\n",
    "\t\t\ta_key = p[a]\n",
    "\t\t\tfor b in range(a+1,len(p)):\n",
    "\t\t\t\tb_key = p[b]\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\ta_in = a_key in meets.keys()\n",
    "\t\t\t\tb_in = b_key in meets.keys()\n",
    "\t\t\t\tif not a_in and not b_in:\n",
    "\t\t\t\t\tmeets[a_key]=set()\n",
    "\t\t\t\t\tmeets[a_key].add(b_key)\n",
    "\t\t\t\telif a_in:\n",
    "\t\t\t\t\tmeets[a_key].add(b_key)\n",
    "\t\t\t\telif b_in:\n",
    "\t\t\t\t\tmeets[b_key].add(a_key)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\traise Exception(\"\")\n",
    "\n",
    "\tedges = []\n",
    "\tfor a in meets.keys():\n",
    "\t\tfor b in meets[a]:\n",
    "\t\t\tedges.append([a,b])\n",
    "\tedges = pd.DataFrame(edges, columns=['A', 'B'])\n",
    "\tedges.to_csv('data/geolife_meet_edges.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ef6d921-fd98-4282-b3ba-22daf12c9f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading\n",
      "starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240746/240746 [00:10<00:00, 23149.52it/s]\n"
     ]
    }
   ],
   "source": [
    "create_meet_edges_without_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe9c27-e496-4eb9-98a0-b48c36023066",
   "metadata": {},
   "source": [
    "# 8. Geohash meets with time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "451c5ddb-d32b-4d40-81c2-6d85700824f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meet_edges(min_seconds, max_seconds, min_times, max_times):\n",
    "\n",
    "\tprint('loading')\n",
    "\tmeet_geohash = pd.read_csv(\"data/geolife_meet_geohash_size_8.csv\")\n",
    "\tmeet_geohash = meet_geohash[(meet_geohash['count_t'] <= max_times) & (meet_geohash['count_t'] >= min_times)].copy(deep=True)\n",
    "\tmeet_geohash.reset_index(drop=True, inplace=True)\n",
    "\tmeets = {}\n",
    "\tprint('starting')\n",
    "\tfor i in tqdm(range(len(meet_geohash))):\n",
    "\t\tusers = json.loads(meet_geohash.iloc[i]['times'])\n",
    "\t\tkeys = list(users.keys())\n",
    "\t\tfor a in range(len(keys)):\n",
    "\t\t\ta_key = keys[a]\n",
    "\t\t\ta_times = np.array(users[a_key], dtype='datetime64[s]')\n",
    "\t\t\tfor b in range(a+1,len(keys)):\n",
    "\t\t\t\tb_key = keys[b]\n",
    "\t\t\t\tb_times = np.array(users[b_key], dtype='datetime64[s]')\n",
    "\n",
    "\t\t\t\tfor a_time in a_times:\n",
    "\t\t\t\t\tfor b_time in b_times:\n",
    "\t\t\t\t\t\tdiff = abs(np.timedelta64(b_time - a_time, 's').astype('int'))\n",
    "\t\t\t\t\t\tif min_seconds <= diff <= max_seconds:\n",
    "\t\t\t\t\t\t\ta_in = a_key in meets.keys()\n",
    "\t\t\t\t\t\t\tb_in = b_key in meets.keys()\n",
    "\t\t\t\t\t\t\tif not a_in and not b_in:\n",
    "\t\t\t\t\t\t\t\tmeets[a_key]=set()\n",
    "\t\t\t\t\t\t\t\tmeets[a_key].add(b_key)\n",
    "\t\t\t\t\t\t\telif a_in:\n",
    "\t\t\t\t\t\t\t\tmeets[a_key].add(b_key)\n",
    "\t\t\t\t\t\t\telif b_in:\n",
    "\t\t\t\t\t\t\t\tmeets[b_key].add(a_key)\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\traise Exception(\"\")\n",
    "\n",
    "\tfor i in meets.keys():\n",
    "\t\tmeets[i]=list(meets[i])\n",
    "\t\t\n",
    "\tif not (os.path.isdir('data/meets/')):\n",
    "\t\t\tos.makedirs('data/meets/')\n",
    "\t\t\t\n",
    "\twith open('data/meets/meets_size_8_'+str(min_seconds)+'-'+str(max_seconds)+'s-'+str(min_times)+'-'+str(max_times)+'times.json', \"w\") as write_file:\n",
    "\t\tjson.dump(meets, write_file)\n",
    "\n",
    "\tedges = []\n",
    "\tfor a in meets.keys():\n",
    "\t\tfor b in meets[a]:\n",
    "\t\t\tedges.append([a,b])\n",
    "\tedges = pd.DataFrame(edges, columns=['A', 'B'])\n",
    "\tedges.to_csv('data/meets/geolife_meet_edges_size_8_'+str(min_seconds)+'-'+str(max_seconds)+'s-'+str(min_times)+'-'+str(max_times)+'times.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "647833af-8b8e-4056-bc85-a952c0af2e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading\n",
      "starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 41156/206349 [00:47<05:58, 461.22it/s] C:\\Users\\danstorm\\AppData\\Local\\Temp\\ipykernel_13384\\1945220256.py:21: RuntimeWarning:\n",
      "\n",
      "overflow encountered in scalar absolute\n",
      "\n",
      "100%|██████████| 206349/206349 [04:28<00:00, 768.84it/s] \n"
     ]
    }
   ],
   "source": [
    "# create_meet_edges(0, 120, 0, 10)\n",
    "create_meet_edges(0, 120, 0, 100)\n",
    "# create_meet_edges(0, 120, 0, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6b058-18c5-4f27-a2ed-010da5eb6587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
