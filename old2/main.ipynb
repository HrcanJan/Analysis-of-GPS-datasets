{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import load_geolife\n",
    "import add_geohash\n",
    "import create_nodes\n",
    "import geohash_location_count\n",
    "import geohash_user_count\n",
    "import create_geohash_meet\n",
    "import create_geohash_meet_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dirs():\n",
    "    if not os.path.isdir('images/'):\n",
    "        os.makedirs('images/')\n",
    "    if not os.path.isdir('htmls/'):\n",
    "        os.makedirs('htmls/')\n",
    "    if not os.path.isdir('data/'):\n",
    "        os.makedirs('data/')\n",
    "    if not os.path.isdir('data2/'):\n",
    "        os.makedirs('data2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDuplicates():\n",
    "    duplicates = pd.read_csv('data/geolife_duplicates_content.csv')\n",
    "    df = pd.read_csv('data/geolife_geohash_size_8.csv')\n",
    "    df['px'] = df['Person ID'].astype('string') + \"_\" + df['Trajectory'].astype('string') + \".plt\"\n",
    "    duplicates['px'] = duplicates['user'].astype('string')+\"_\"+duplicates['file_name'].astype('string')\n",
    "    noduplicates = df[~df['px'].isin(duplicates['px'])].copy(deep=True)\n",
    "    noduplicates.to_csv('data/geolife_geohash_size_8_no_duplicates.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/182 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [02:55<00:00,  1.04it/s]\n",
      "100%|██████████| 18165801/18165801 [08:47<00:00, 34445.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n",
      "415946\n",
      "grouping...\n",
      "grouped\n",
      "1.0 415946 / 415946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414417/414417 [55:03<00:00, 125.46it/s] \n"
     ]
    }
   ],
   "source": [
    "create_dirs()\n",
    "load_geolife.load_geolife('./geolife/Data/')\n",
    "add_geohash.add_geohash()\n",
    "\n",
    "removeDuplicates()\n",
    "create_nodes.create_nodes()\n",
    "geohash_location_count.geohash_location_count()\n",
    "geohash_user_count.geohash_user_count()\n",
    "count_files = create_geohash_meet.createGeohashMeetJsons()\n",
    "create_geohash_meet.createGeohashMeetCSV(count_files)\n",
    "\n",
    "create_geohash_meet_time.create_geohash_time_meets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "177227it [00:24, 2207.76it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, '2009-03-01 14:02:57', '2009-05-11 08:23:03', 'wx4ex1g0', 7952590, 7952598] [24, '2009-03-25 23:54:45', '2009-03-25 23:54:45', 'wx4ex1g0', 9795864, 9795864]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "415946it [00:40, 10146.00it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_geohash_time_meets_local():\n",
    "\tdf = pd.read_csv('data/geolife_geohash_meet_intervals_size_8.csv')\n",
    "\tmeets_dict = {}\n",
    "\n",
    "\tfor index, row in tqdm(df.iterrows()):\n",
    "\t\tmeets_intervals = json.loads(row['meets_intervals'])\n",
    "\t\t\n",
    "\t\tif len(meets_intervals) > 1:\n",
    "\t\t\tkeys = list(meets_intervals.keys())\n",
    "\t\t\tintervals = [meets_intervals[key] for key in keys]\n",
    "\t\t\t\n",
    "\t\t\tfor i in range(len(keys)):\n",
    "\t\t\t\tfor j in range(i + 1, len(keys)): \n",
    "\t\t\t\t\tfor interval_i in intervals[i]:\n",
    "\t\t\t\t\t\tfor interval_j in intervals[j]:\n",
    "\t\t\t\t\t\t\tif (interval_i[1] <= interval_j[2] and interval_j[1] <= interval_i[2]):\n",
    "\t\t\t\t\t\t\t\tif((keys[i] == '24' or keys[i] == '4') and (keys[j] == '24' or keys[j] == '4')):\n",
    "\t\t\t\t\t\t\t\t\tprint(interval_i, interval_j)\n",
    "\t\t\t\t\t\t\t\tkey = tuple(sorted([keys[i], keys[j]]))\n",
    "\t\t\t\t\t\t\t\tmeets_dict.setdefault(row['geohash'], set()).add(key)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tkey = next(iter(meets_intervals))\n",
    "\t\t\tmeets_dict.setdefault(row['geohash'], set()).add(key)\n",
    "\n",
    "\tnew_data = [{'geohash': geohash, 'person_ids': list(person_pairs)} for geohash, person_pairs in meets_dict.items()]\n",
    "\tnew_df = pd.DataFrame(new_data)\n",
    "\n",
    "\tnew_df.to_csv('data2/geolife_local_meets.csv', index=False)\n",
    "\n",
    "create_geohash_time_meets_local()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "415946it [00:44, 9443.09it/s] \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_geohash_time_meets_global():\n",
    "\tdf = pd.read_csv('data/geolife_geohash_meet_intervals_size_8.csv')\n",
    "\tmeets_dict = {}\n",
    "\n",
    "\tfor index, row in tqdm(df.iterrows()):\n",
    "\t\tmeets_intervals = json.loads(row['meets_intervals'])\n",
    "\t\t\n",
    "\t\tif len(meets_intervals) > 1:\n",
    "\t\t\tkeys = list(meets_intervals.keys())\n",
    "\t\t\tintervals = [meets_intervals[key] for key in keys]\n",
    "\t\t\t\n",
    "\t\t\tfor i in range(len(keys)):\n",
    "\t\t\t\tfor j in range(i + 1, len(keys)): \n",
    "\t\t\t\t\tfor interval_i in intervals[i]:\n",
    "\t\t\t\t\t\tfor interval_j in intervals[j]:\n",
    "\t\t\t\t\t\t\tif (interval_i[1] <= interval_j[2] and interval_j[1] <= interval_i[2]):\n",
    "\t\t\t\t\t\t\t\tkey = tuple(sorted([keys[i], keys[j]]))\n",
    "\t\t\t\t\t\t\t\tmeets_dict.setdefault(key, {'users': key, 'meetings': [], 'geohashes': set(), 'time': [None, None]})\n",
    "\t\t\t\t\t\t\t\tmeets_dict[key]['meetings'].append((row['Latitude'], row['Longitude']))\n",
    "\t\t\t\t\t\t\t\tmeets_dict[key]['geohashes'].add(row['geohash'])\n",
    "\t\t\t\t\t\t\t\tif meets_dict[key]['time'][0] is None or interval_i[1] < meets_dict[key]['time'][0]:\n",
    "\t\t\t\t\t\t\t\t\tmeets_dict[key]['time'][0] = interval_i[1]\n",
    "\t\t\t\t\t\t\t\tif meets_dict[key]['time'][1] is None or interval_j[2] > meets_dict[key]['time'][1]:\n",
    "\t\t\t\t\t\t\t\t\tmeets_dict[key]['time'][1] = interval_j[2]\n",
    "\n",
    "\tnew_data = [{'users': info['users'], 'meetings': info['meetings'], 'geohashes': list(info['geohashes']), 'time': info['time']} for info in meets_dict.values()]\n",
    "\tnew_df = pd.DataFrame(new_data)\n",
    "\n",
    "\tnew_df.to_csv('data2/geolife_global_meets.csv', index=False)\n",
    "\n",
    "create_geohash_time_meets_global()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
