{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import load_geolife\n",
    "import add_geohash\n",
    "import create_nodes\n",
    "import geohash_location_count\n",
    "import geohash_user_count\n",
    "import create_geohash_meet\n",
    "import create_geohash_intervals\n",
    "import geohash_pairs\n",
    "import find_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dirs():\n",
    "    if not os.path.isdir('images/'):\n",
    "        os.makedirs('images/')\n",
    "    if not os.path.isdir('htmls/'):\n",
    "        os.makedirs('htmls/')\n",
    "    if not os.path.isdir('data/'):\n",
    "        os.makedirs('data/')\n",
    "    if not os.path.isdir('data/duplicates'):\n",
    "        os.makedirs('data/duplicates')\n",
    "    if not os.path.isdir('data/geolife_geohash_intervals'):\n",
    "        os.makedirs('data/geolife_geohash_intervals')\n",
    "    if not os.path.isdir('data/geolife_pairs'):\n",
    "        os.makedirs('data/geolife_pairs')\n",
    "\n",
    "\n",
    "def removeDuplicates(in_file, duplicates_file, out_file):\n",
    "    duplicates = pd.read_csv(duplicates_file)\n",
    "    df = pd.read_csv(in_file)\n",
    "    df['px'] = df['Person ID'].astype('string') + \"_\" + df['Trajectory'].astype('string') + \".plt\"\n",
    "    duplicates['px'] = duplicates['user'].astype('string')+\"_\"+duplicates['file_name'].astype('string')\n",
    "    noduplicates = df[~df['px'].isin(duplicates['px'])].copy(deep=True)\n",
    "    noduplicates.to_csv(out_file,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5046480/5046480 [1:08:02<00:00, 1236.26it/s]\n"
     ]
    }
   ],
   "source": [
    "create_dirs()\n",
    "# load_geolife.load_geolife('./geolife/Data/',\"./data/geolife_gps_data.csv\", \"./data/geolife_labels_data.csv\")\n",
    "# add_geohash.add_geohash('./data/geolife_gps_data.csv', './data/geolife_geohash_size_8.csv')\n",
    "# find_duplicates.find_duplicates_by_filename('./geolife/Data/', './data/duplicates/')\n",
    "# find_duplicates.find_duplicates_by_content('./geolife/Data/', './data/duplicates/')\n",
    "# removeDuplicates('./data/geolife_geohash_size_8.csv', './data/duplicates/geolife_user_track_duplicates_by_content.csv', './data/geolife_geohash_size_8_no_duplicates.csv')\n",
    "# create_nodes.create_nodes('./data/geolife_geohash_size_8_no_duplicates.csv', './data/geolife_nodes.csv')\n",
    "# geohash_location_count.geohash_location_count('./data/geolife_geohash_size_8_no_duplicates.csv', './data/geolife_geohash_8_location_counts.csv')\n",
    "# geohash_user_count.geohash_user_count('./data/geolife_geohash_size_8_no_duplicates.csv', './data/geolife_geohash_8_user_counts.csv')\n",
    "# count_files = create_geohash_meet.createGeohashMeetJsons('./data/geolife_geohash_size_8_no_duplicates.csv', './data/geolife_geohash_meet/')\n",
    "# create_geohash_meet.createGeohashMeetCSV(count_files,'./data/geolife_geohash_meet/', './data/geolife_geohash_meet_size_8.csv')\n",
    "# create_geohash_intervals.create_geohash_intervals('./data/geolife_geohash_meet_size_8.csv', './data/geolife_geohash_meet_intervals_size_8.csv')\n",
    "# geohash_pairs.transform_times('./data/geolife_geohash_meet_intervals_size_8.csv','./data/geolife_geohash_intervals/','./data/geolife_geohash_8_intervals_table.csv')\n",
    "# geohash_pairs.split_to_single_day('./data/geolife_geohash_8_intervals_table.csv','./data/geolife_geohash_8_intervals_single_day.csv')\n",
    "# geohash_pairs.create_date_pairs('./data/geolife_geohash_8_intervals_single_day.csv','./data/geolife_geohash_8_day_meets.csv')\n",
    "geohash_pairs.create_pairs('./data/geolife_geohash_8_intervals_single_day.csv','./data/geolife_pairs/', './data/geolife_geohash_8_pairs.csv')\n",
    "geohash_pairs.create_edges('./data/geolife_geohash_8_pairs.csv', './data/geolife_geohash_size_8_edges.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "415946it [00:43, 9624.37it/s] \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_geohash_time_meets_local():\n",
    "    df = pd.read_csv('data/geolife_geohash_meet_intervals_size_8.csv')\n",
    "    meets_dict = {}\n",
    "\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        meets_intervals = json.loads(row['meets_intervals'])\n",
    "        \n",
    "        if len(meets_intervals) > 1:\n",
    "            keys = list(meets_intervals.keys())\n",
    "            intervals = [meets_intervals[key] for key in keys]\n",
    "            \n",
    "            for i in range(len(keys)):\n",
    "                for j in range(i + 1, len(keys)): \n",
    "                    for interval_i in intervals[i]:\n",
    "                        for interval_j in intervals[j]:\n",
    "                            if (interval_i[1] <= interval_j[2] and interval_j[1] <= interval_i[2]):\n",
    "                                key = tuple(sorted([keys[i], keys[j]]))\n",
    "                                # Add the meeting information to the dictionary, including the geohash and timestamp\n",
    "                                meets_dict.setdefault((row['geohash'], interval_i[5]), set()).add(key)\n",
    "\n",
    "        else:\n",
    "            key = next(iter(meets_intervals))\n",
    "            # Add the meeting information to the dictionary, including the geohash and timestamp\n",
    "            meets_dict.setdefault((row['geohash'], meets_intervals[key][0][5]), set()).add(key)\n",
    "\n",
    "    # Flatten the dictionary to create the DataFrame\n",
    "    new_data = [{'geohash': key[0], 'time': key[1], 'person_ids': list(person_pairs)} for key, person_pairs in meets_dict.items()]\n",
    "    new_df = pd.DataFrame(new_data)\n",
    "\n",
    "    new_df.to_csv('data2/geolife_local_meets.csv', index=False)\n",
    "\n",
    "create_geohash_time_meets_local()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "415946it [00:44, 9266.63it/s] \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_geohash_time_meets_global():\n",
    "    df = pd.read_csv('data/geolife_geohash_meet_intervals_size_8.csv')\n",
    "    meets_dict = {}\n",
    "\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        meets_intervals = json.loads(row['meets_intervals'])\n",
    "        \n",
    "        if len(meets_intervals) > 1:\n",
    "            keys = list(meets_intervals.keys())\n",
    "            intervals = [meets_intervals[key] for key in keys]\n",
    "            \n",
    "            for i in range(len(keys)):\n",
    "                for j in range(i + 1, len(keys)): \n",
    "                    for interval_i in intervals[i]:\n",
    "                        for interval_j in intervals[j]:\n",
    "                            if (interval_i[1] <= interval_j[2] and interval_j[1] <= interval_i[2]):\n",
    "                                key = tuple(sorted([keys[i], keys[j]]))\n",
    "                                # Use the start time of interval_i for timestamp\n",
    "                                timestamp = interval_i[5]  # Assuming start time is at index 5\n",
    "                                meets_dict.setdefault(key, {'users': key, 'meetings': [], 'geohashes': set(), 'time': [None, None]})\n",
    "                                meets_dict[key]['meetings'].append((row['Latitude'], row['Longitude']))\n",
    "                                meets_dict[key]['geohashes'].add(row['geohash'])\n",
    "                                if meets_dict[key]['time'][0] is None or interval_i[1] < meets_dict[key]['time'][0]:\n",
    "                                    meets_dict[key]['time'][0] = interval_i[1]\n",
    "                                if meets_dict[key]['time'][1] is None or interval_j[2] > meets_dict[key]['time'][1]:\n",
    "                                    meets_dict[key]['time'][1] = interval_j[2]\n",
    "\n",
    "    new_data = [{'users': info['users'], 'meetings': info['meetings'], 'geohashes': list(info['geohashes']), 'time': info['time']} for info in meets_dict.values()]\n",
    "    new_df = pd.DataFrame(new_data)\n",
    "\n",
    "    new_df.to_csv('data2/geolife_global_meets.csv', index=False)\n",
    "\n",
    "create_geohash_time_meets_global()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
